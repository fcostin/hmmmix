\documentclass[twoside, 11pt]{article}

\usepackage{jmlr2e}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}

% ensure sufficient marginspace for todos
\setlength {\marginparwidth }{2cm}
\usepackage[obeyFinal]{todonotes}
% \setuptodonotes{inline}

% define notation for norm and abs that scale nicely.
% ref: https://tex.stackexchange.com/a/297263
\let\oldnorm\norm
\let\norm\undefined
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\let\oldabs\abs
\let\abs\undefined
\DeclarePairedDelimiter\abs{\lvert}{\rvert}

\DeclarePairedDelimiter\card{\lvert}{\rvert}

\newcommand{\xx}[0] {\mathbb{X}} % decision variable space
\newcommand{\hh}[0] {\mathbb{H}} % stochastic process for state
\newcommand{\zz}[0] {\mathbb{Z}} % hidden event space?
\newcommand{\mm}[0] {\mathbb{M}} % HMM model type space
\newcommand{\TT}[0] {\mathbb{T}} % time indices
\newcommand{\II}[0] {\mathbb{I}} % factorial model indices
\newcommand{\traj}[1] {H^{(#1)}}
\newcommand{\state}[2] {H_{#2}^{(#1)}}
\newcommand{\event}[2] {Z_{#2}^{(#1)}}
\newcommand{\eventseq}[1] {Z^{(#1)}}
\newcommand{\reals}[0] {\mathbb{R}}
\newcommand{\naturals}[0] {\mathbb{N}}
\newcommand{\events}[0] {\mathbb{Y}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\author{\name Reuben Fletcher-Costin}

\editor{}

\title{Approximate MAP inference of sparse factorial hidden Markov models through set cover decomposition}

\maketitle

\begin{abstract}%
Our goal is to infer a sparse subset of hidden Markov models (HMMs) that collectively explain a sequence of observations. We define a probabilistic model for a simple form of factorial HMMs and pose a maximum a posteriori (MAP) estimation problem to infer a sparse subset of component HMM, their hidden states, and separate the observations into component signals associated with each component HMM. We show that the MAP estimation problem is equivalent to an exact cover problem, where each component HMM is regarded as a set that covers a portion of the observed signal. This permits a relaxed approximation of the problem to be decomposed as a master set cover problem and auxiliary problem using column generation. Each auxiliary problem can be solved efficiently using dynamic programming by a "prize-collecting" modified Viterbi algorithm that recovers a HMM trajectory that incorporates prizes from the master problem for explaining portions of the observed signal.
\end{abstract}

% keywords could go here

\section{Introduction}

\todo[inline]{why care about HMMs}
\todo[inline]{why care about separation problems}
\todo[inline]{discuss factorial HMMs}
\todo[inline]{discuss alternatives to factorial HMMs}
\todo[inline]{intro linear programming decompositions, convex optimisation decompositions}

\section{Probabilistic model}

\subsection{Hidden Markov models}
We start by assuming a family of hidden Markov models, indexed by $m \in \mm$. Each model $m \in \mm$ in the family has a finite state space $S_m = \{s_1, \ldots, s_{K_m}\}$ consisting of $K_m \in \naturals$ states. The state of the model $m$ at time $t \in \TT = \{ 1, \ldots, T \}$ is denoted by the random variable $H_{m, t}$ which takes values in the state space $S_m$. We regard the states as not directly observable and refer to them as hidden (aka latent) states. The hidden states of model $m$ evolve in time independently from other models according to a discrete time first-order Markov process
\begin{equation}
P(H_{m,t+1} \mid \{ H_{m^{\prime},t^{\prime}} \}_{m^{\prime}, t^{\prime} \in \mm \times \TT \setminus \{m, t\}} )
=
P(H_{m,t+1} \mid H_{m,t} )
\end{equation}
We assume that the transition model $P(H_{m,t+1} \mid H_{m,t} )$ for each $m \in \mm$ is stationary, that is, there exists some $K_m$ by $K_m$ stochastic matrix $A^{m}$ with elements $A^{m}_{s^{\prime}, s}$ such that for all $t \in \TT$, $A^{m}_{s^{\prime}, s} = P(H_{m, t+1}=s^{\prime} \mid H_{m,t}=s)$. We write $\pi_m$ to denote a prior distribution over $H_{m,1}$ at $t=1$. In the simple case where we believe a-priori that the $m$th HMM is present \footnote{Once we consider a sparse factorial model composed of multiple component HMMs from the family $\mm$, we may no longer believe a-priori that all or any HMM $m \in \mm$ necessarily participates in the factorial model.}, we could write $P(H_{m,1}) = \pi_m$.

Each hidden Markov model $m$ emits a sequence of signals $Z_{m,1}, \ldots, Z_{m,T}$, where each $Z_{m,t} \in \events$. The space of signals $\events$ is assumed to be a vector space over $\reals$. Each signal $Z_{m,t}$ is assumed to be caused solely by the corresponding hidden state $H_{m,t}$:
\begin{equation}
P\left(Z_{m,t}
\mid
\{ H_{m^{\prime},t^{\prime}} \}_{m^{\prime}, t^{\prime} \in \mm \times \TT} \;
\{ Z_{m^{\prime},t^{\prime}} \}_{m^{\prime}, t^{\prime} \in \mm \times \TT \setminus \{m, t\}}
\right)
=
P(Z_{m,t} \mid H_{m,t} ) .
\end{equation}
We refer to $P(Z_{m, t} \mid H_{m,t} )$ as the observation model for the $m$th model. We assume the observation model for each $m \in \mm$ is stationary with respect to $t$.


\todo{add a clear graphical model of a single HMM for some $m \in \mm$}

\subsection{Factorial hidden Markov models}

We consider factorial hidden Markov models consisting of a multiset of component hidden Markov models, where the possible component HMMs are indexed by $i$ over some abstract index set $\II$. Note that the cardinality of $\II$ may not be finite. We defer explictly constructing $\II$ until section \ref{section:inference}. Let $m(i) \in \mm$ denote the component HMM associated with the $i$th index. We represent a given factorial model as the pair $X, M$, where
\begin{align}
X & := (X_i)_{i \in \II} \quad X_i \in \{0, 1\} , \\
M & := (m(i))_{i \in II} \quad m(i) \in \mm .
\end{align}
We regard $X$ as a collection of random variables over binary values that address a subset of indices from $\II$, and $M$ as a collection of random variables over $\mm$ that associate a particular type of HMM $m(i) \in \mm$ to each selected index. Note this representation of factorial models is not unique to permutations of the indices $\II$.

We restrict our focus to \emph{sparse} factorial models where $\norm{X}_1$ is bounded, that is, where there exists $n \in \naturals$ such that $\sum_{i \in \II} \abs{X_i} \leq n$. We can further express a preference for sparsity through a choice of prior over $X$.

We are interested in estimating a factorial hidden Markov model with components $(X_i)_{i \in \II}$ that best "explains" some given observed data $Y_t \in \events$ for $t \in \TT$. The standard probabilistic model for a single HMM $m \in \mm$ assumes that at each time $t in \TT$ the output signal $Z_{m,t}$ is directly observable. In contrast, with a factorial HMM the output $Z_{m,t}$ of each component HMM $m$ is not directly observable. Instead, we observe a signal $Y_t$ each $t \in \TT$ that aggregates the hidden output signals of all component HMMs:
\begin{equation}
Y_t = \sum_{i \in \II} X_i \event i t ,
\end{equation}
where $\event i t := Z_{m(i), t}$. Note that unlike the definition of the more general factorial hidden Markov model considered by {Ghahramani and Jordan 1997}\todo{CITE} we assume there is no top-level error term and assume a different dependency structure between $Y$ and $Z$. Error terms can be expressed by $\event i t$ through the observation models of the component HMMs.

\todo{add graphical model of this factorial HMM construction using Y X Z M H}

\section{Inference} \label{section:inference}

\todo{rework in terms of index set $\II$. that will allow multiple copies of a single class of markov model to appear in solution. that's fine, as long as they each pay their way by justifying another copy of the log prior in the objective function}

Our goal is to infer a sparse factorial HMM, represented by $X$ and $M$, that explains a sequence of outputs $Y = (Y_t)_{t \in \TT}$. Given the framework of Bayesian inference, this amounts to computing the posterior distribution $P(X, M | Y=y)$ given observed data $y \in \events^{\TT}$. Computing this posterior distribution exactly is computationally challenging as it requires integrating over all the hidden states $\state i t$ and all outputs $\event i t$ for all $t \in \TT$ and all potential component models $i \in \II$. A less useful but more computationally feasible task is to instead compute a maximum a posteriori parameter estimate of the quad $(X, M, H, Z) = (X_i, m(i), \traj i, Z^{(i)})_{i \in \II}$ given the observed data $Y=y$. The elements of the quad define which component models participate in the factorial model ($X$), the type of each component from the family $\mm$ ($M$), the hidden states of each component ($H$), and the hidden outputs emitted by each component ($Z$).

This form of MAP estimate is computationally tractable because the probabilities of each component HMM state $(X_i, Z_i)$ are conditionally independent from $(X_j, Z_j)$ $j \neq i$ given $Z$, allowing conditional probabilities to be calculated independently component by component once some value of $Z$ is fixed. This leads to a tractable decomposition. \todo{explain in terms of causal graph diagram, markov blankets, d separation}

We demonstrate the decomposition more formally. Consider the following factorisation of the conditional probability of the variables $X, H, Z$ given the data $Y$:
\begin{align}
P(X, M, H, Z \mid Y)
& = P(Y \mid X, M, H, Z) P(X, M, H, Z) / P(Y) \label{map1} \\
& \propto P(Y \mid X, M, H, Z) P(X, M, H, Z) \label{map2} \\
& = P(Y \mid X, Z) P(X, M, H, Z) \label{map3} \\
& = P(Y \mid X, Z) P(Z | X, M, H) P(X, M, H) \label{map4} \\
& = P\left(Y \mid X, Z\right) \prod_{i \in \II} P\left(\eventseq i | M_i, \traj i\right)^{X_i} P(X, M, H) \label{map5} \\
& = P\left(Y = \sum_{i \in \II} X_i \eventseq i \mid X, Z \right)
\prod_{i \in \II} P\left(\eventseq i | M_i, \traj i\right)^{X_i} P(X, M, H) \label{map6}
\end{align}
\todo{fixup p a given b bar scaling}
where \ref{map1} uses Bayes' theorem, \ref{map2} drops the factor $P(Y)$ as it is invariant during maximisation over $(X, H, Z)$, \ref{map3} applies the conditional independence of $Y$ from $H$ and $M$ given $X$ and $Z$, \ref{map5} is due to the conditional independence of the component observation models given $Z$, and \ref{map6} applies the definition of the aggregated observation $Y$ from the component hidden outputs.

To complete the decomposition of the conditional probability $P(X, M, H, Z \mid Y)$ in terms the distributions defined by the family of component Markov processes, we need to decide on $P(X, M, H)$, a prior joint distribution over $X$, $M$ and $H$. We assume the prior probabilities of $\traj m$ and $\traj {m^{\prime}}$ are conditionally independent given $X$ for each distinct $m, m^{\prime} \in \mm$. Therefore we have
\begin{align}
P(X, M, H)
& = \prod_{i \in \II} P(\traj i \mid m(i)) P(m(i) \mid X_i ) P(X) \\
& = \prod_{i \in \II} \left( \frac{1}{\card{\mm}} P(\traj i \mid m(i)) \right)^{X_i} P(X) \\
\end{align}
where $P(\traj i \mid m(i))$ is the prior probability of the trajectory $\traj {m(i)} = \pi_{m(i)}$ as supplied by the component HMM. The $\frac{1}{\card{\mm}}$ factor is due to assuming a uniform prior over $m(i)$ that does not prefer any particular model $m$ of the family $\mm$. We tentatively define a prior $P(X)$ over the binary sequence $X = \{X_i\}_{i \in \II} \in \{0, 1\}^{\card{I}}$ in terms of a prior $P(n)$ over the number of active processes $n := \norm{X}_1$ that assigns higher prior probability to explanations involving fewer active Markov processes:
\begin{align}
P(X)
& = P(X \mid n) P(n) \\
& \propto \frac{1}{|\mm|^n \; 2^{n+1}}
\end{align}
where we have omitted a normalisation constant required so that $\sum_{X} P(X) = 1$
\todo{rework, support nonuniform prior on $\mm$ as it is easy and probably quite useful in applications}

\todo{add $M$ into the argmax}

Consider the MAP parameter estimation problem:
\begin{equation}
(X^{\star}, M^{\star}, H^{\star}, Z^{\star}) = \argmax_{X, M, H, Z} P\left(X, M, H, Z \mid Y\right)
\end{equation}
Substituting our decomposition of $P(X, M, H, Z | Y)$ and the definition of our prior $P(X, M, H)$ gives
\begin{align*}
& \argmax_{X, M, H, Z} P\left(X, H, Z \mid Y\right) \\
= & \argmax_{X, M, H, Z}
P\left(Y = \sum_{i \in \II} X_i \eventseq i \mid X, Z \right)
\prod_{i \in \mm} P\left(\eventseq i | m(i), \traj i\right)^{X_i} P(X, M, H) \\
= & \argmax_{X, M, H, Z} \prod_{i \in \II} P\left(\eventseq i | m(i), \traj i\right)^{X_i}
\prod_{i \in \II} P(\traj i \mid m(i))^{X_i}
\frac{1}{|\mm|^n \; 2^{n+1}} \\
 & \mathrm{s.t.} \sum_{i \in \II} X_i \eventseq i = Y
\end{align*}
where $n = \norm{X}_1$ and the factor for the conditional probability of the observation vector $Y$ is equivalently expressed by the constraining the max to consider only those quads $(X, M, H, Z)$ such that $\sum_{i \in \II} X_i \eventseq i = Y$.

Since $\argmax$ is invariant under transformation of the objective function by a monotonic function, by the monotonicity of the logarithm we have
\begin{align}
(X^{\star}, M^{\star}, H^{\star}, Z^{\star})
= & \argmax_{X, M, H, Z} \sum_{i \in \II} X_i C(m(i), \eventseq i, \traj i) \\
 & \mathrm{s.t.} \sum_{i \in \II} X_i \eventseq i = Y ,
\end{align}
where $C(m(i), \eventseq i, \traj i)$ is defined by
\begin{equation*}
C(m(i), \eventseq i, \traj i) =
\log P\left(\eventseq i | m(i), \traj i\right) + 
\log P(\traj i \mid m(i)) -
\left ( \log |\mm| + \log 2 \right) ,
\end{equation*}
and we have dropped terms from the $\argmax$ that are constant
with respect to $(X, M, H, Z)$.

\todo{explicitly construct the index and show this is a linear program}

\section{Practical considerations}

{bootstrapping initial feasible solution so there is a dual solution that can be used to set initial prizes}\todo{elabourate}

{decomposition of MAP estimation problem into master relaxed exact cover problem and auxiliary prize-collecting Viterbi dynamic programming problem allows many complex problem specific details to be handled as part of the dynamic programming}\todo{elabourate}

{degeneracy / slow convergence of restricted relaxed master linear program}\todo{investigate and mitigate}

{exactly when is the dual solution used to set prizes well defined -- over which range?}\todo{investigate}

{efficient search over particular families of HMMs}\todo{investigate}

{embed into full branch and price regime}\todo{investigate}

{reducing wasted effort by reusing LP bookkeeping and prior soln to warm-start over successive LP solves}\todo{investigate}

\section{Variations}

{maybe hard equality constraint linking Y to Z is not pragmatic. if soften constraint and allow nonzero error $r$ one naive implementation might add factor $k \exp(-\frac{\norm{r}^2}{{\sigma^2}})$ to unlogged objective function. equivalently when objective is logged, this adds some $- \norm{r}^2$ term. seems to give some kind of quadratic optimisation problem. can decomposition still work?}\todo{investigate}

\section{bibliography}
\todo{setup bibtex etc}

\end{document}
