\documentclass[twoside, 11pt]{article}

\usepackage{jmlr2e}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}

% ensure sufficient marginspace for todos
\setlength {\marginparwidth }{2cm}
\usepackage[obeyFinal]{todonotes}
% \setuptodonotes{inline}

% define notation for norm and abs that scale nicely.
% ref: https://tex.stackexchange.com/a/297263
\let\oldnorm\norm
\let\norm\undefined
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\let\oldabs\abs
\let\abs\undefined
\DeclarePairedDelimiter\abs{\lvert}{\rvert}

\newcommand{\xx}[0] {\mathbb{X}} % decision variable space
\newcommand{\hh}[0] {\mathbb{H}} % stochastic process for state
\newcommand{\zz}[0] {\mathbb{Z}} % hidden event space?
\newcommand{\mm}[0] {\mathbb{M}} % HMM model type space
\newcommand{\TT}[0] {\mathbb{T}} % time indices
\newcommand{\II}[0] {\mathbb{I}} % factorial model indices
\newcommand{\traj}[1] {H^{(#1)}}
\newcommand{\state}[2] {H_{#2}^{(#1)}}
\newcommand{\event}[2] {Z_{#2}^{(#1)}}
\newcommand{\eventseq}[1] {Z_{(#1)}}
\newcommand{\reals}[0] {\mathbb{R}}
\newcommand{\naturals}[0] {\mathbb{N}}
\newcommand{\events}[0] {\mathbb{Y}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\author{\name Reuben Fletcher-Costin}

\editor{}

\title{Approximate MAP inference of sparse factorial hidden Markov models through set cover decomposition}

\maketitle

\begin{abstract}%
Our goal is to infer a sparse subset of hidden Markov models (HMMs) that collectively explain a sequence of observations. We define a probabilistic model for a simple form of factorial HMMs and pose a maximum a posteriori (MAP) estimation problem to infer a sparse subset of component HMM, their hidden states, and separate the observations into component signals associated with each component HMM. We show that the MAP estimation problem is equivalent to an exact cover problem, where each component HMM is regarded as a set that covers a portion of the observed signal. This permits a relaxed approximation of the problem to be decomposed as a master set cover problem and auxiliary problem using column generation. Each auxiliary problem can be solved efficiently using dynamic programming by a "prize-collecting" modified Viterbi algorithm that recovers a HMM trajectory that incorporates prizes from the master problem for explaining portions of the observed signal.
\end{abstract}

% keywords could go here

\section{Introduction}

\todo[inline]{why care about HMMs}
\todo[inline]{why care about separation problems}
\todo[inline]{discuss factorial HMMs}
\todo[inline]{discuss alternatives to factorial HMMs}
\todo[inline]{intro linear programming decompositions, convex optimisation decompositions}

\section{Probabilistic model}

\subsection{Hidden Markov models}
We start by assuming a family of hidden Markov models, indexed by $m \in \mm$. Each model $m \in \mm$ in the family has a finite state space $S_m$ consisting of $K_m \in \naturals$ states $\{s_1, \ldots, s_{K_m}\}$. The state of the model $m$ at time $t \in \TT = \{ 1, \ldots, T \}$ is denoted by the random variable $H_{m, t}$ which takes values in the state space $S_m$. We regard the states as not directly observable and refer to them as hidden (aka latent) states. The hidden states of model $m$ evolve in time independently from other models according to a discrete time first-order Markov process
\begin{equation}
P(H_{m,t+1} \mid \{ H_{m^{\prime},t^{\prime}} \}_{m^{\prime}, t^{\prime} \in \mm \times \TT \setminus \{m, t\}} )
=
P(H_{m,t+1} \mid H_{m,t} )
\end{equation}
We assume that the transition model $P(H_{m,t+1} \mid H_{m,t} )$ for each $m \in \mm$ is stationary, that is, there exists some stochastic matrix $A^{m}$ with elements $A^{m}_{s^{\prime}, s} = P(H_{m, t+1}=s^{\prime} \mid H_{m,t}=s)$. We write $\pi_m$ to denote a prior distribution over $H_{m,1}$ at $t=1$.

Each hidden Markov model $m$ emits a sequence of events $Z_{m,1}, \ldots, Z_{m,T}$, where each $Z_{m,t} \in \events$. The space of events $\events$ is assumed to be a vector space over $\reals$. Each $Z_{m,t}$ is assumed to be caused solely by the corresponding hidden state $H_{m,t}$:
\begin{equation}
P\left(Z_{m,t}
\mid
\{ H_{m^{\prime},t^{\prime}} \}_{m^{\prime}, t^{\prime} \in \mm \times \TT} \;
\{ Z_{m^{\prime},t^{\prime}} \}_{m^{\prime}, t^{\prime} \in \mm \times \TT \setminus \{m, t\}}
\right)
=
P(Z_{m,t} \mid H_{m,t} ) .
\end{equation}
We refer to $P(Z_{m, t} \mid H_{m,t} )$ as the observation model for the $m$th model. We assume the observation model for each $m \in \mm$ is stationary with respect to $t$.

\subsection{Factorial hidden Markov model}

We consider factorial hidden Markov models consisting of a multiset of component hidden Markov models, where the possible component HMMs are indexed by $i \in \II$. Let $m(i) \in \mm$ denote the component HMM associated with the $i$th index. We represent a particular factorial model by the sequence $(X_i)_{i \in \II}$, where each $X_i$ can be regarded as a random variable over the states $\{0, 1\}$. A value of $X_i=1$ indicates that the component HMM $m(i) \in \mm$ participates in the factorial model represented by $X = (X_i)_{i \in \II}$ . We are interested in identifying a factorial hidden Markov model with components $(X_i)_{i \in \II}$ that explains some given observed signal $Y_t \in \events$ for $t \in \TT$. We restrict our focus to sparse factorial models where $\norm{X}_1$ is bounded, that is, there exists $n \in \naturals$ such that $\sum_{i \in \II} \abs{X_i} \leq n$. Note that the cardinality of the abstract index set $\II$ may not be finite. We defer explictly constructing $\II$. 

Recall the standard case of a single HMM with a hidden state that is assumed to output a directly observable signal at each time step. In contrast, with a factorial HMM the output of each component HMM is not directly observable. Instead we are able to observe a signal $Y_t$ each $t \in \TT$ that aggregates contributions from the output signals of all component HMMs:
\begin{equation}
Y_t = \sum_{i \in \II} X_i \event i t ,
\end{equation}
where we have introduced $\event i t := Z_{m(i), t}$. Note that unlike the definition of the factorial hidden Markov model given by {Ghahramani and Jordan 1997}\todo{CITE} we assume there is no error term. Error terms can be expressed by $\event i t$ through the observation models of the component HMMs.

\section{Inference}

\todo{rework in terms of index set $\II$. that will allow multiple copies of a single class of markov model to appear in solution. that's fine, as long as they each pay their way by justifying another copy of the log prior in the objective function}

\todo{fixup notation to use $Z^(i)$ not $Z_(m)$}


Our goal is to infer a sparse subset of Markov processes that collectively explain a sequence of observations $Y = (Y_t)_{t \in \TT}$. Ideally we wish to estimate the posterior distribution $P(X | Y)$ of $X = (X_i)_{i \in \II}$. This is computationally challenging as it involves integrating over the states of all hidden trajectories $\{ \traj m\}_{m \in \mm}$ and potential hidden events $\{ Z_m \}_{m \in \mm}$. A much more tractable task is to instead compute a maximum a posteriori parameter estimate of the triple $(X, H, Z) = (X_m, \traj m, Z_m)_{m \in \mm}$ given the observed data $Y$.

To see why this is more tractable, consider the following factorisation of the conditional probability of the variables $X, H, Z$ given the data $Y$:
\begin{align}
P(X, H, Z \mid Y)
& = P(Y \mid X, H, Z) P(X, H, Z) / P(Y) \label{map1} \\
& \propto P(Y \mid X, H, Z) P(X, H, Z) \label{map2} \\
& = P(Y \mid X, Z) P(X, H, Z) \label{map3} \\
& = P(Y \mid X, Z) P(Z | X, H) P(X, H) \label{map4} \\
& = P\left(Y \mid X, Z\right) \prod_{m \in \mm} P\left(\eventseq m | \traj m\right)^{X_m} P(X, H) \label{map5} \\
& = P(Y = \sum_{m \in \mm} X_m \eventseq m \mid X, Z ) \prod_{m \in \mm} P\left(\eventseq m | \traj m\right)^{X_m} P(X, H) \label{map6}
\end{align}
where \ref{map1} uses Bayes' theorem, \ref{map2} drops the factor $P(Y)$ as it is invariant during maximisation over $(X, H, Z)$, \ref{map3} applies the conditional independence of $Y$ from $H$ given $X$ and $Z$, \ref{map5} is due to the conditional independence of the observation models given $Z$ implied by equation \ref{obsmodelindep}, and \ref{map6} applies the definition of $Y$.

To complete the decomposition of the conditional probability $P(H, H, Z \mid Y)$ in terms the distributions defined by the family of component Markov processes, we need to decide on $P(X, H)$, the prior for $X$ and $H$. We assume the prior probabilities of $\traj m$ and $\traj {m^{\prime}}$ are conditionally independent given $X$ for each distinct $m, m^{\prime} \in \mm$. Therefore we have
\begin{align}
P(X, H)
& = \prod_{m \in \mm} P(\traj m \mid X_m ) P(X) \\
& = \prod_{m \in \mm} P(\traj m)^{X_m} P(X)
\end{align}
where $P(\traj m)$ is the prior probability of the trajectory $\traj m$ of hidden states of the $m$th Markov process. We tentatively define a prior $P(X)$ over the bit vector $\{X_m\}_{m \in \mm} \in \{0, 1\}^{|\mm|}$ in terms of a prior $P(n)$ over the number of active processes $n = \sum_{m \in \mm} X_m$ that assigns higher prior probability to explanations involving fewer active Markov processes:
\begin{align}
P(X)
& = P(X \mid n) P(n) \\
& \propto \frac{1}{|\mm|^n \; 2^{n+1}}
\end{align}
where we have omitted a normalisation constant required so that $\sum_{X} P(X) = 1$

Consider the MAP parameter estimation problem:
\begin{equation}
(X^{\star}, H^{\star}, Z^{\star}) = \argmax_{X, H, Z} P\left(X, H, Z \mid Y\right)
\end{equation}
Substituting our decomposition of $P(X, H, Z | Y)$ and the definition of our prior $P(H, X)$ gives
\begin{align*}
& \argmax_{X, H, Z} P\left(X, H, Z \mid Y\right) \\
= & \argmax_{X, H, Z} P(Y = \sum_{m \in \mm} X_m \eventseq m \mid X, Z ) \prod_{m \in \mm} P\left(\eventseq m | \traj m\right)^{X_m} P(X, H) \\
= & \argmax_{X, H, Z} \prod_{m \in \mm} P\left(\eventseq m | \traj m\right)^{X_m}
\prod_{m \in \mm} P(\traj m)^{X_m}
\frac{1}{|\mm|^n \; 2^{n+1}} \\
 & \mathrm{s.t.} \sum_{m \in \mm} X_m \eventseq m = Y
\end{align*}
where $n = \sum_m X_m$ and the factor for the conditional probability of the observation vector $Y$ is equivalently expressed by the constraining the max to consider only those triples $(X, H, Z)$ such that $\sum_{m \in \mm} X_m \eventseq m = Y$.

We note that $\argmax$ is invariant under transformation of the objective function by a monotonic function, so by the monotonicity of the logarithm we have
\begin{align}
(X^{\star}, H^{\star}, Z^{\star})
= & \argmax_{X, H, Z} \sum_{m \in \mm} X_m C(\eventseq m, \traj m) \\
 & \mathrm{s.t.} \sum_{m \in \mm} X_m \eventseq m = Y ,
\end{align}
where $C(\eventseq m, \traj m)$ is defined by
\begin{equation*}
C(\eventseq m, \traj m) =
\log P\left(\eventseq m | \traj m\right) + 
\log P(\traj m) -
\left ( \log |\mm| + \log 2 \right) ,
\end{equation*}
and we have dropped terms from the $\argmax$ that are constant
with respect to $(X, H, Z)$.

\todo{bibiliography}

\end{document}
